{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5000, 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000, 0.5000]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "cigan  = torch.tensor([[[1.,1.,1.,1.],[1.,1.,1.,1.],[1.,1.,1.,1.]],[[1.,1.,1.,1.],[1.,1.,1.,1.],[1.,1.,1.,1.]]])\n",
    "print(cigan.softmax(dim = 0))\n",
    "print(cigan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Softmax(data , axis = None): \n",
    "        \n",
    "    z = data - np.max(data)\n",
    "    rom = np.exp(z)\n",
    "    print(f\"rom shape  = {rom.shape}\")\n",
    "    output_T = rom / np.sum(rom, axis = axis)\n",
    "\n",
    "    return output_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rom shape  = (2, 3, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,3,4) (12,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[413], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m esk \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[[\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m],[\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m],[\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m]],[[\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m],[\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m],[\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m]]])\n\u001b[1;32m----> 2\u001b[0m out \u001b[39m=\u001b[39m Softmax(esk, axis \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(out)\n",
      "Cell \u001b[1;32mIn[392], line 6\u001b[0m, in \u001b[0;36mSoftmax\u001b[1;34m(data, axis)\u001b[0m\n\u001b[0;32m      4\u001b[0m rom \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(z)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrom shape  = \u001b[39m\u001b[39m{\u001b[39;00mrom\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m output_T \u001b[39m=\u001b[39m rom \u001b[39m/\u001b[39;49m np\u001b[39m.\u001b[39;49msum(rom, axis \u001b[39m=\u001b[39;49m axis)\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m output_T\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3,4) (12,1) "
     ]
    }
   ],
   "source": [
    "esk = np.array([[[1.,1.,1.,1.],[1.,1.,1.,1.],[1.,1.,1.,1.]],[[1.,1.,1.,1.],[1.,1.,1.,1.],[1.,1.,1.,1.]]])\n",
    "out = Softmax(esk, axis = 0)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(esk, axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Mini.Tensor import Tensor\n",
    "#rom = Tensor([[1,1,1,1],[1,1,1,1],[1,1,1,1]])\n",
    "#\n",
    "#out = Softmax(rom, axis = 1)\n",
    "#print(out)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "T1 = np.random.randn(4,4)\n",
    "T2 = np.random.randn(4,4)\n",
    "T3 = np.random.randn(4,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[344], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m first_ \u001b[39m=\u001b[39m t1_\u001b[39m.\u001b[39mdot(t2_)                                 \n\u001b[0;32m      9\u001b[0m second_ \u001b[39m=\u001b[39m first_ \u001b[39m+\u001b[39m t3_                             \n\u001b[1;32m---> 10\u001b[0m third_ \u001b[39m=\u001b[39msecond_\u001b[39m.\u001b[39;49mSoftmax(axis \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m )                               \n\u001b[0;32m     11\u001b[0m LOSS \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(true_hat \u001b[39m*\u001b[39m (third_\u001b[39m.\u001b[39mlog()))\u001b[39m.\u001b[39msum()                                \n\u001b[0;32m     12\u001b[0m LOSS\u001b[39m.\u001b[39mbackward()                                               \n",
      "File \u001b[1;32mc:\\Users\\pavle\\MiniFrameworkV2\\Mini\\Tensor.py:174\u001b[0m, in \u001b[0;36mTensor.Softmax\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mSoftmax\u001b[39m(\u001b[39mself\u001b[39m, axis \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):      \u001b[39m# https://stackoverflow.com/questions/42599498/numerically-stable-softmax https://math.stackexchange.com/questions/2843505/derivative-of-softmax-without-cross-entropy\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m-\u001b[39m \u001b[39mmax\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata), axis \u001b[39m=\u001b[39m axis)\n\u001b[0;32m    175\u001b[0m     softmax_ \u001b[39m=\u001b[39m z \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(z)\n\u001b[0;32m    177\u001b[0m     output_T \u001b[39m=\u001b[39m Tensor(softmax_)\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "true_hat = Tensor([[1], [0], [0], [0]])\n",
    "\n",
    "torch.manual_seed(100)\n",
    "t1_ = Tensor(T1)                                         \n",
    "t2_ = Tensor(T2)                                        \n",
    "t3_ = Tensor(T3)  * 0.1                                 \n",
    "\n",
    "first_ = t1_.dot(t2_)                                 \n",
    "second_ = first_ + t3_                             \n",
    "third_ =second_.Softmax(axis = 0 )                               \n",
    "LOSS = -(true_hat * (third_.log())).sum()                                \n",
    "LOSS.backward()                                               \n",
    "print(LOSS)\n",
    "print(third_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor = [[0.0749  0.01202 0.06371 0.47772]\n",
      " [0.03095 0.08097 0.0063  0.01651]\n",
      " [0.0172  0.01037 0.01095 0.00366]\n",
      " [0.09303 0.04134 0.02075 0.03961]]>\n"
     ]
    }
   ],
   "source": [
    "print(third_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                               LOG_SOFTMAX                                 #\n",
    "true_hat_1  = torch.Tensor([[1], [0], [0], [0]])\n",
    "torch.manual_seed(100)\n",
    "t1_1 = torch.randn((4,4))                                        ;t1_1.requires_grad = True \n",
    "t2_1 = torch.randn((4,1))                                        ;t2_1.requires_grad = True\n",
    "t3_1 = torch.randn((4,1))  * 0.1                                 ;t3_1.requires_grad = True\n",
    "\n",
    "\n",
    "first_1 = t1_1.matmul(t2_1)                                        ; first_1.retain_grad()\n",
    "second_1 = first_1 + t3_1                                          ; second_1.retain_grad()\n",
    "third_1 =second_1.softmax(dim = 0)                                ; third_1.retain_grad()                     \n",
    "loss_2 = -torch.sum(true_hat_1 * torch.log(third_1))\n",
    "\n",
    "loss_2.backward()                                                       ;loss_2.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(third_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6821, grad_fn=<NegBackward0>)\n",
      "tensor([[-5.3766],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(loss_2)\n",
    "print(third_1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.07968],\n",
       "       [11.90041],\n",
       "       [20.62231],\n",
       "       [61.90045]], dtype=float32)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_third = third_1.detach().numpy()\n",
    "t = third_1.detach().numpy()\n",
    "\n",
    "softmax_der  = np.dot(np.diagflat(t)  -t.T * t, der_third) \n",
    "softmax_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5945],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000]])\n",
      "tensor([[ 0.4839],\n",
      "        [-0.2869],\n",
      "        [-0.1746],\n",
      "        [-0.0224]])\n"
     ]
    }
   ],
   "source": [
    "print(third_1.grad)\n",
    "print(second_1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "true_hat = torch.tensor([1.,0.,0.,0.])\n",
    "#true_hat = torch.Tensor([1,0,0,0])\n",
    "\n",
    "torch.manual_seed(100)\n",
    "t1_ = torch.randn((4,4))                                        ;t1_.requires_grad = True \n",
    "t2_ = torch.randn((4,1))                                        ;t2_.requires_grad = True\n",
    "t3_ = torch.randn((4,1))  * 0.1                                 ;t3_.requires_grad = True\n",
    "\n",
    "\n",
    "first_ = t1_.matmul(t2_)                                        ; first_.retain_grad()\n",
    "second_ = first_ + t3_                                          ; second_.retain_grad()\n",
    "third_ =second_.softmax(dim = 0)                               ; third_.retain_grad()  \n",
    "loss = loss_fn(second_.squeeze(), true_hat)                                 ; loss.retain_grad()\n",
    "loss.backward()                                               \n",
    "print(LOSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
