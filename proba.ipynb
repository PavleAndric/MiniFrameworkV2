{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "np.set_printoptions(precision = 5)  # this is ugly here\n",
    "\n",
    "class Tensor():\n",
    "\n",
    "    def __init__(self, data, _children = ()):\n",
    "        \n",
    "        self.data = data if isinstance(data, (np.ndarray, np.generic)) else np.array(data, dtype = np.float32)\n",
    "        self.shape = self.data.shape\n",
    "        self._backward = lambda : None\n",
    "        self.prev = set(_children)\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Tensor data = {self.data}>\"\n",
    "    \n",
    "    def shape(self)-> Tuple[int]: return self.shape\n",
    "\n",
    "    def size(self)-> int: return self.data.size\n",
    "    #-                                            BINARY                                                 -\n",
    "    def __add__(self, other )-> 'Tensor': \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        output_T  = Tensor(self.data + other.data, (self,other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output_T.grad \n",
    "            other.grad += output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "\n",
    "    def __mul__(self, other)-> 'Tensor': \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        output_T = Tensor(self.data * other.data,(self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * output_T.grad \n",
    "            other.grad += self * output_T.grad\n",
    "    \n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def __pow__(self, other) -> 'Tensor': #https://testbook.com/learn/maths-derivative-of-exponential-function\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        output_T = Tensor(self.data ** other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self ** (other - 1)) * output_T.grad\n",
    "            other.grad += output_T * self.log() * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "\n",
    "    def __sub__(self, other)-> 'Tensor':\n",
    "        other  = other if isinstance(other , Tensor) else Tensor(other)\n",
    "        output_T = Tensor(self.data - other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output_T.grad\n",
    "            other.grad += -output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T \n",
    "\n",
    "    def __radd__(self, other) -> 'Tensor':\n",
    "        return self + other\n",
    "    \n",
    "    def __rmul__(self, other)-> 'Tensor':\n",
    "        return self * other \n",
    "    \n",
    "    def __rsub__(self, other)-> 'Tensor':\n",
    "        return other + (self * -1)\n",
    "    \n",
    "    def __truediv__(self, other)-> 'Tensor':\n",
    "        return self * (other **-1)\n",
    "    \n",
    "    def __rtruediv__(self, other)-> 'Tensor':\n",
    "        return other * (self**-1)\n",
    "    #-                                             UNARY      math                                     -\n",
    "    def sum(self) -> 'Tensor':\n",
    "        output_T = Tensor(self.data.sum(), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += Tensor.ones_like(self) * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def log(self)-> 'Tensor':\n",
    "        output_T = Tensor(np.log(self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += Tensor.ones_like(self) / self * output_T.grad\n",
    "        \n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def mean(self)-> 'Tensor':\n",
    "        output_T = Tensor(np.mean(self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            t = Tensor.ones_like(self)  \n",
    "            self.grad += t / self.size() * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def sqrt(self)-> 'Tensor':\n",
    "        output_T = Tensor(np.sqrt(self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 / (2 * output_T) * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    #                                            UNARY transformation                                          -\n",
    "    def __neg__(self)-> 'Tensor':      # TODO this may couse errors \n",
    "        return self * -1 \n",
    "\n",
    "    def abs(self) -> 'Tensor':   \n",
    "        output_T =  Tensor(np.abs(self.data), (self, ))\n",
    "        def _backward():\n",
    "            self.grad += Tensor(np.sign(self.data)) * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T \n",
    "    # TODO write T.grad more efficiently               \n",
    "    def T(self) -> 'Tensor':\n",
    "        output_T = Tensor(np.transpose(self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            \n",
    "            self.grad += Tensor(np.transpose(np.inner(output_T.grad.data, np.ones_like(self.data))))   #TODO find a nicer way to do this\n",
    "\n",
    "        output_T._backward  = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def unsqueeze(self, axis) -> 'Tensor':\n",
    "        return Tensor(np.expand_dims(self.data, axis = axis))\n",
    "    \n",
    "    #                                                DOT                                                     - \n",
    "    def dot(self, other) -> 'Tensor':\n",
    "        other = other if isinstance(other , Tensor) else Tensor(other)\n",
    "\n",
    "        output_T  = Tensor(np.dot(self.data, other.data), (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += Tensor(output_T.grad.data.dot(other.data.T))\n",
    "            other.grad +=  Tensor(self.data.T.dot(output_T.grad.data))\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    #                                               Activation functions                                      - \n",
    "    def ReLU(self):\n",
    "        output_T = Tensor(np.maximum(0, self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += Tensor(output_T.data > 0) * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def Sigmoid(self):\n",
    "\n",
    "        exp = np.exp(-self.data)\n",
    "        output_T = Tensor((1/(1 + exp)), (self, )) # der_sig (1/(1 + np.exp(-input))* 1- 1/(1 + np.exp(-input)))\n",
    "\n",
    "        def _backwrad():\n",
    "            self.grad += Tensor(output_T.data - output_T.data**2) * output_T.grad \n",
    "\n",
    "        output_T._backward = _backwrad\n",
    "        return output_T\n",
    "    \n",
    "    def Tanh(self):\n",
    "        output_T = Tensor(np.tanh(self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = Tensor(1- output_T.data**2) * output_T.grad\n",
    "        \n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def Softmax(self):      # https://stackoverflow.com/questions/42599498/numerically-stable-softmax\n",
    "\n",
    "        z = self.data - max(self.data)\n",
    "        o = np.exp(z)\n",
    "        softmax = o / np.sum(o)\n",
    "\n",
    "        output_T = Tensor(softmax)\n",
    "        def _backward():\n",
    "            self.grad \n",
    "        return output_T\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape)-> 'Tensor': return cls(np.zeros(shape))\n",
    "        \n",
    "    @classmethod\n",
    "    def ones(cls, shape)-> 'Tensor': return cls(np.ones(shape))\n",
    "\n",
    "    @classmethod\n",
    "    def ones_like(cls, Tensor)-> 'Tensor' : return cls(np.ones(Tensor.shape))\n",
    "       \n",
    "    @classmethod\n",
    "    def zeros_like(cls, Tensor)-> 'Tensor': return cls(np.zeros(Tensor.shape))\n",
    "\n",
    "    RNG = np.random.default_rng() #https://numpy.org/doc/stable/reference/random/generator.html\n",
    "    @classmethod\n",
    "    def randn(cls, shape)-> 'Tensor': return cls(Tensor.RNG.standard_normal(size = shape))\n",
    "        \n",
    "    @classmethod\n",
    "    def uniform(cls, shape)-> 'Tensor': return cls(Tensor.RNG.uniform(low = -1 , high =  1, size = shape))\n",
    "       \n",
    "    @classmethod\n",
    "    def arange(cls, start, stop, step)-> 'Tensor': return cls(np.arange(start = start, stop = stop , step = step ))\n",
    "\n",
    "    #-                                           ENGINE                                                  -\n",
    "    def backward(self):\n",
    "        \n",
    "        topo = []\n",
    "        visited = set()\n",
    "    \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = Tensor([1.0])\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor data = [0.03206 0.08714 0.23688 0.64391]>\n"
     ]
    }
   ],
   "source": [
    "t = Tensor([1,2,3,4])\n",
    "p = t.Softmax()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0321, 0.0871, 0.2369, 0.6439])\n"
     ]
    }
   ],
   "source": [
    "m = torch.nn.Softmax(dim=0)\n",
    "input = torch.Tensor([1,2,3,4])\n",
    "output = m(input)\n",
    "L = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor data = [0.23482 0.24367 0.25437 0.26715]>\n"
     ]
    }
   ],
   "source": [
    "t1 = Tensor([1,2,3,4]) * 0.2\n",
    "t2 = Tensor([5,6,7,8]) * 0.1\n",
    "t3 = Tensor([9,10,11,12]) * 0.01\n",
    "\n",
    "first = t1 + t2\n",
    "second = first * t3\n",
    "third = second.Softmax()\n",
    "L = third.sum()\n",
    "print(third)\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_hat = torch.tensor([1.,0.,0.,0.])\n",
    "#true_hat = torch.Tensor([1,0,0,0])\n",
    "\n",
    "torch.manual_seed(100)\n",
    "t1_ = torch.randn((4,4))                                        ;t1_.requires_grad = True \n",
    "t2_ = torch.randn((4,1))                                        ;t2_.requires_grad = True\n",
    "t3_ = torch.randn((4,1))  * 0.1                                 ;t3_.requires_grad = True\n",
    "\n",
    "\n",
    "first_ = t1_.matmul(t2_)                                        ; first_.retain_grad()\n",
    "second_ = first_ + t3_                                          ; second_.retain_grad()\n",
    "third_ =second_.softmax(dim = 0)                                ; third_.retain_grad()  \n",
    "loss = loss_fn(second_.squeeze(), true_hat)                                 ; loss.retain_grad()\n",
    "loss.backward()                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.3766],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(third_.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4230,  0.2246,  0.7941, -0.3899],\n",
      "        [ 0.8437, -0.1331, -0.4708,  0.2312],\n",
      "        [ 0.5134, -0.0810, -0.2865,  0.1407],\n",
      "        [ 0.0659, -0.0104, -0.0368,  0.0181]])\n"
     ]
    }
   ],
   "source": [
    "print(t1_.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8140,  0.4826,  0.2937,  0.0377], grad_fn=<SubBackward0>)\n",
      "tensor([-0.8455,  0.4010,  0.2440,  0.0313], grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# der of softmax = si(1 - sj)\n",
    "\n",
    "#n = np.size(self.out)\n",
    "#ret = np.dot((np.identity(n) - self.out.T) * self.out, o_grad)\n",
    "n = 4 #third_.size()\n",
    "der_CLE = third_.squeeze() - true_hat\n",
    "ret  = torch.matmul(torch.eye(4) - torch.transpose(third_, 0, 1) * third_, der_CLE)\n",
    "print(der_CLE) # THIS IS IT\n",
    "#print(ret)\n",
    "# der of CEL is s - y  (s is the predictied ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4230,  0.2246,  0.7941, -0.3899],\n",
      "        [ 0.8437, -0.1331, -0.4708,  0.2312],\n",
      "        [ 0.5134, -0.0810, -0.2865,  0.1407],\n",
      "        [ 0.0659, -0.0104, -0.0368,  0.0181]])\n"
     ]
    }
   ],
   "source": [
    "print(t1_.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6821, grad_fn=<DivBackward1>) tensor(1.6821, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "loss_ = -torch.sum(true_hat * torch.log(third_.squeeze()))\n",
    "print(loss, loss_)\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "t1_1 = torch.randn((4,4))                                        ;t1_1.requires_grad = True \n",
    "t2_1 = torch.randn((4,1))                                        ;t2_1.requires_grad = True\n",
    "t3_1 = torch.randn((4,1))  * 0.1                                 ;t3_1.requires_grad = True\n",
    "\n",
    "\n",
    "first_1 = t1_1.matmul(t2_1)                                        ; first_1.retain_grad()\n",
    "second_1 = first_1 + t3_1                                          ; second_1.retain_grad()\n",
    "third_1 =second_1.softmax(dim = 0)                                ; third_1.retain_grad()                     ; loss.retain_grad()\n",
    "loss_2 = -torch.sum(true_hat * torch.log(third_1.squeeze()))\n",
    "\n",
    "loss_2.backward()                                                 ; loss_2.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[-5.3766],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000]])\n",
      "tensor([[-0.8140],\n",
      "        [ 0.4826],\n",
      "        [ 0.2937],\n",
      "        [ 0.0377]])\n",
      "tensor([[-0.8140],\n",
      "        [ 0.4826],\n",
      "        [ 0.2937],\n",
      "        [ 0.0377]])\n"
     ]
    }
   ],
   "source": [
    "print(loss_2.grad)\n",
    "print(third_1.grad)\n",
    "print(second_1.grad)\n",
    "print(t3_1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.3766, -0.0000, -0.0000, -0.0000], grad_fn=<MulBackward0>)\n",
      "tensor([-5.1906,  0.4826,  0.2937,  0.0377], grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "der_third  =  (-1 / (third_1.squeeze()) * true_hat.squeeze())  *  1#(loss_grad is one) ## MINUS CUZ THE SUM IS MINUS\n",
    "print(der_third)\n",
    "der_second  = torch.matmul(torch.eye(4) - torch.transpose(third_, 0, 1) * third_, der_third) #torch.matmul(torch.eye(4) - torch.transpose(second_1, 0, 1) * second_1.squeeze(), der_third)\n",
    "print(der_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.1906,  0.4826,  0.2937,  0.0377], grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "third_1.squeeze() * (1- third_1.squeeze()) * der_third\n",
    "print(torch.matmul(torch.eye(4) - torch.transpose(third_, 0 , 1) * third_, der_third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4230,  0.2246,  0.7941, -0.3899],\n",
      "        [ 0.8437, -0.1331, -0.4708,  0.2312],\n",
      "        [ 0.5134, -0.0810, -0.2865,  0.1407],\n",
      "        [ 0.0659, -0.0104, -0.0368,  0.0181]])\n"
     ]
    }
   ],
   "source": [
    "print(t1_1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4669,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.9097, -0.0000,  0.0000,  0.0000],\n",
       "        [ 1.9097,  0.0000, -0.0000,  0.0000],\n",
       "        [ 1.9097,  0.0000,  0.0000, -0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.eye(4)\n",
    "k = third_1.squeeze().dot((third_1).squeeze())\n",
    "(t- k)* der_third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
