{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    " \n",
    "class Tensor():\n",
    "\n",
    "    def __init__(self, data, _children = ()):\n",
    "\n",
    "        self.data = np.array(data, dtype =np.float32)\n",
    "        self.shape = self.data.shape\n",
    "        self._backward = lambda : None\n",
    "        self.prev = set(_children)\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Tensor data = {self.data}>\"\n",
    "    \n",
    "    def shape(self): return self.shape\n",
    "\n",
    "    def size(self): return self.data.size\n",
    "    #-                                            BINARY                                                 -\n",
    "    def __add__(self, other): \n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        output_T  = Tensor(self.data + other.data, (self,other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output_T.grad   # chain rule \n",
    "            other.grad += output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "\n",
    "    def __mul__(self, other): \n",
    "\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        output_T = Tensor(self.data * other.data,(self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * output_T.grad \n",
    "            other.grad += self * output_T.grad\n",
    "    \n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        output_T = Tensor(self.data ** other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self ** (other - 1)) * output_T.grad\n",
    "        \n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        other  = other if isinstance(other , Tensor) else Tensor(other)\n",
    "        \n",
    "        output_T = Tensor(self.data - other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output_T.grad\n",
    "            other.grad += -output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T \n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other \n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return other + (self * -1)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other **-1)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * (self**-1)\n",
    "    #-                                             UNARY                                               -\n",
    "    def sum(self):\n",
    "        output_T = Tensor(self.data.sum(), (self, ))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = Tensor.ones_like(self) * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def log(self):\n",
    "        output_T = Tensor(np.log(self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = Tensor.ones_like(self) / self * output_T.grad\n",
    "        \n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def mean(self):\n",
    "        output_T = Tensor(np.mean(self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            t = Tensor.ones_like(self)  \n",
    "            self.grad = t / self.size() * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def sqrt(self):\n",
    "        output_T = Tensor(np.sqrt(self.data), (self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = 1 / (2 * output_T) * output_T.grad\n",
    "\n",
    "        output_T._backward = _backward\n",
    "        return output_T\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1 \n",
    "\n",
    "    @classmethod\n",
    "    def zeros(cls, shape): return cls(np.zeros(shape))\n",
    "        \n",
    "    @classmethod\n",
    "    def ones(cls , shape):  return cls(np.ones(shape))\n",
    "\n",
    "    @classmethod\n",
    "    def ones_like(cls,Tensor) : return cls(np.ones(Tensor.shape))\n",
    "       \n",
    "    @classmethod\n",
    "    def zeros_like(cls, Tensor): return cls(np.zeros(Tensor.shape))\n",
    "\n",
    "    RNG = np.random.default_rng() #https://numpy.org/doc/stable/reference/random/generator.html\n",
    "    @classmethod\n",
    "    def randn(cls, shape): return cls(Tensor.RNG.standard_normal(size = shape))\n",
    "        \n",
    "    @classmethod\n",
    "    def uniform(cls, shape):   return cls(Tensor.RNG.uniform(low = -1 , high =  1, size = shape))\n",
    "       \n",
    "    @classmethod\n",
    "    def arange(cls, start, stop, step): return cls(np.arange(start = start, stop = stop , step = step ))\n",
    "\n",
    "    #-                                           ENGINE                                                  -\n",
    "    def backward(self):\n",
    "        \n",
    "        topo = []\n",
    "        visited = set()\n",
    "    \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = Tensor([1.0])\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tensor([[0.6],[0.2]])\n",
    "k = Tensor([[1.2],[0.3]])\n",
    "o = Tensor([[2.0],[3.0]])\n",
    "q = Tensor([[1.4], [0.3]])\n",
    "\n",
    "first  = t + k\n",
    "second = o * first\n",
    "third = second.sqrt()\n",
    "forth = third ** q\n",
    "L = forth.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o.grad <Tensor data = [[0.42899257]\n",
      " [0.02656768]]>\n",
      "t,grad <Tensor data = [[0.4766584]\n",
      " [0.1594061]]>\n",
      "k.grad <Tensor data = [[0.4766584]\n",
      " [0.1594061]]>\n",
      " q_grad = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"o.grad {o.grad}\")\n",
    "print(f\"t,grad {t.grad}\")\n",
    "print(f\"k.grad {k.grad}\")\n",
    "print(f\" q_grad = {q.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75daff32a5d0a89c32da8fe186cc95eda36c57939ef32a1dc2d40fa77430b921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
